# Classification on MNIST Dataset
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import mnist

# Load MNIST data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Preprocessing
x_train = x_train.reshape(-1, 28, 28, 1)  # Shape: (60000, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)
x_train = x_train / 255.0
x_test = x_test / 255.0

# Data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    zoom_range=0.2,
)

# Build model
model = Sequential([
    Flatten(input_shape=(28, 28, 1)),
    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # L2 Regularization
    Dropout(0.5),  # Dropout Layer
    Dense(10, activation='softmax')
])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Compile model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train model
history = model.fit(datagen.flow(x_train, y_train),
                    epochs=10,
                    validation_data=(x_test, y_test),
                    callbacks=[early_stopping])

# ------------------------------------------------------

# Regression on Iris Dataset
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

# Load dataset
df = sns.load_dataset('iris')
print(df.head())

# Feature and target separation
x = df.drop(columns=['sepal_length'])
y = df['sepal_length']

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Label Encoding species column
le = LabelEncoder()
x_train['species'] = le.fit_transform(x_train['species'])
x_test['species'] = le.transform(x_test['species'])

# Add noise to features
noise_factor = 0.1
x_train_noisy = x_train + noise_factor * np.random.normal(size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(size=x_test.shape)

# Build regression model
reg_model = Sequential([
    Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1)
])

# Early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Compile regression model
reg_model.compile(optimizer=Adam(learning_rate=0.001),
                  loss='mean_squared_error',
                  metrics=['mae'])

# Train regression model
history = reg_model.fit(x_train_noisy, y_train,
                        epochs=25,
                        validation_data=(x_test_noisy, y_test),
                        callbacks=[early_stopping])

# Evaluate the regression model
y_pred = reg_model.predict(x_test_noisy)
print("\nR2 Score:", r2_score(y_test, y_pred))

# Plot training history
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Model Loss (Regression)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()
